{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# python ./c_morl.py --env=cenp --gamma=0.997 --noise=0.7 --pref=0.5 --seed=123\n",
    "\n",
    "# In[1]:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40) # remove gym warning about float32 bound box precision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import common.env_lib\n",
    "from common.env_utils import sorl_plot\n",
    "from common.rl_lib import ReplayBuffer, OUNoise, ENoise, ValueNetwork, PolicyNetwork, ddpg_update, calc_values_of_states\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", default=\"csense\", type=str, help=\"Environment. Specified in ./common/env_lib.py\")\n",
    "parser.add_argument(\"--gamma\", default=0.996, type=float, help=\"Discount Factor\")\n",
    "parser.add_argument(\"--noise\", default=0.8, type=float, help=\"Action Noise\")\n",
    "parser.add_argument(\"--seed\", default=238, type=int, help=\"Set seed [default: 238]\")\n",
    "parser.add_argument(\"--pref\", default=0.5, type=float, help=\"Set preference [default: 0.5]\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "arguments\n",
    "seed      = args.seed\n",
    "env_name  = args.env\n",
    "GAMMA     = args.gamma\n",
    "max_noise = args.noise\n",
    "pref      = args.pref\n",
    "\n",
    "# arguments\n",
    "# seed      = 160861\n",
    "# env_name  = \"cenp\"\n",
    "# GAMMA     = 0.997\n",
    "# max_noise = 0.7\n",
    "# pref      = 0.1\n",
    "intrp_no  = 4\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# set seed\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "################################################################################\n",
    "# If using GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "################################################################################\n",
    "# Setup experiment parameters\n",
    "timeslots_per_day = 24\n",
    "REQ_TYPE = \"random\"\n",
    "\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\")\n",
    "\n",
    "# Type of RL training used for the model that we are going to load\n",
    "training_experiment = \"off_policy_g\" + str(GAMMA) + \"-n\" + str(max_noise) + \"-random_pref\" + \"-intrp\" + str(intrp_no)\n",
    "################################################################################\n",
    "\n",
    "# Tags for ENP model\n",
    "enp_env_tag = env_name + '_t' + str(timeslots_per_day) + '_' + REQ_TYPE\n",
    "# name of folder to load model from\n",
    "enp_model_folder = enp_env_tag  + \"-\" + training_experiment \n",
    "\n",
    "# model filename\n",
    "enp_model_tag   = enp_model_folder +'-' + str(seed)\n",
    "enp_pmodel_file = os.path.join(model_folder, enp_model_folder, (enp_model_tag + \"-enp_policy.pt\"))\n",
    "enp_qmodel_file = os.path.join(model_folder, enp_model_folder, (enp_model_tag + \"-enp_value.pt\"))\n",
    "################################################################################\n",
    "\n",
    "# Tags for sense model\n",
    "sense_env_tag = env_name + '_t' + str(timeslots_per_day) + '_' + REQ_TYPE\n",
    "# name of folder to load model from\n",
    "sense_model_folder = sense_env_tag  + \"-\" + training_experiment \n",
    "\n",
    "# model filename\n",
    "sense_model_tag   = sense_model_folder + '-' + str(seed)\n",
    "sense_pmodel_file = os.path.join(model_folder, sense_model_folder, (sense_model_tag + \"-sense_policy.pt\"))\n",
    "sense_qmodel_file = os.path.join(model_folder, sense_model_folder, (sense_model_tag + \"-sense_value.pt\"))\n",
    "\n",
    "################################################################################\n",
    "# Setting up runtime environment base\n",
    "experiment = training_experiment + \"-p\" + str(pref)\n",
    "env = eval(\"common.env_lib.\"+env_name+\"sense\"+\"()\")\n",
    "env.set_pref(pref) # preference over actions\n",
    "################################################################################\n",
    "\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "hidden_dim = 256\n",
    "\n",
    "# Make model\n",
    "sense_pnet = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "enp_pnet = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "sense_qnet = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "enp_qnet = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "sense_pnet.load_state_dict(torch.load(sense_pmodel_file))\n",
    "sense_pnet.eval()\n",
    "\n",
    "sense_qnet.load_state_dict(torch.load(sense_qmodel_file))\n",
    "sense_qnet.eval()\n",
    "\n",
    "enp_pnet.load_state_dict(torch.load(enp_pmodel_file))\n",
    "enp_pnet.eval()\n",
    "\n",
    "enp_qnet.load_state_dict(torch.load(enp_qmodel_file))\n",
    "enp_qnet.eval();\n",
    "\n",
    "# Tags for morl_runtime experiment\n",
    "env_tag = env_name + '_t' + str(timeslots_per_day) + '_' + REQ_TYPE\n",
    "model_tag = experiment +'-'+str(seed)\n",
    "\n",
    "# experiment tag\n",
    "# name of folder to save models and results\n",
    "exp_tag = env_tag  + \"-\" + experiment \n",
    "\n",
    "# experiment+seed tag\n",
    "# tensorboard tag / model filename\n",
    "tag     = env_tag  + \"-\" + experiment +'-'+str(seed) \n",
    "print(\"Experiment TAG: \",tag)\n",
    "\n",
    "# Folder/file to save test results\n",
    "test_results_folder = os.path.join(cur_folder,\"results\", exp_tag, \"test\")\n",
    "if not os.path.exists(test_results_folder): \n",
    "        os.makedirs(test_results_folder) \n",
    "test_log_file = os.path.join(test_results_folder, tag + '-test.npy')    \n",
    "\n",
    "# Setup environment\n",
    "env_location_list = ['tokyo']#['tokyo','wakkanai','minamidaito']\n",
    "START_YEAR = 1995\n",
    "NO_OF_YEARS = 24\n",
    "# timeslots_per_day = 24\n",
    "# REQ_TYPE = \"random\"\n",
    "prediction_horizon = 10*timeslots_per_day\n",
    "henergy_mean= 0.13904705134356052 # 10yr hmean for tokyo\n",
    "\n",
    "exp_test_log = {} # dictionary to store test results\n",
    "for env_location in env_location_list:\n",
    "#     print(env_location)\n",
    "    exp_test_log[env_location] = {}\n",
    "    for year in range(START_YEAR, START_YEAR+NO_OF_YEARS):\n",
    "        exp_test_log[env_location][year]={}\n",
    "        \n",
    "        env.set_env(env_location,year, timeslots_per_day, \n",
    "                    REQ_TYPE, offset=timeslots_per_day/2,\n",
    "                    p_horizon=prediction_horizon,\n",
    "                    hmean=henergy_mean)    \n",
    "        state = env.reset()\n",
    "        reward_rec = []\n",
    "        \n",
    "        intrp_actions_rec = []\n",
    "        intrp_sense_value_rec = []\n",
    "        intrp_enp_value_rec = []\n",
    "        intrp_final_value_rec = []\n",
    "        \n",
    "        ep_done_rec = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.RECOVERY_MODE:\n",
    "                no_action = 0            \n",
    "                next_state, reward, done, _ = env.step(no_action)       \n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "                    # sense_model# sense_model: get action and q-value\n",
    "                    # convert action to DC\n",
    "                    raw_sense_action_tensor = sense_pnet(state_tensor)\n",
    "                    sense_value = sense_qnet(state_tensor, raw_sense_action_tensor).cpu().item()\n",
    "                    raw_sense_action = raw_sense_action_tensor.cpu().item()\n",
    "\n",
    "                    # enp_model: get action and q-value\n",
    "                    # convert action to DC\n",
    "                    enp_action_tensor = enp_pnet(state_tensor)\n",
    "                    enp_value = enp_qnet(state_tensor, enp_action_tensor).cpu().item()\n",
    "                    raw_enp_action = enp_action_tensor.cpu().item()\n",
    "\n",
    "                # Find two intermediate DC\n",
    "                intrp_actions = np.linspace(start=min(raw_sense_action,raw_enp_action), \n",
    "                                                stop=max(raw_sense_action,raw_enp_action), \n",
    "                                                num=intrp_no)\n",
    "                intrp_actions_rec.append(intrp_actions)\n",
    "\n",
    "                # Convert intrp_actions to action\n",
    "                intrp_action_tensor = torch.FloatTensor(intrp_actions).unsqueeze(1).to(device)\n",
    "                batch_state_tensor = torch.FloatTensor([state]*intrp_no).to(device)\n",
    "\n",
    "                # Get the q-values from sense and enp models for the intrp_actions\n",
    "                intrp_sense_value = sense_qnet(batch_state_tensor, intrp_action_tensor).cpu().detach().numpy().squeeze()\n",
    "                intrp_enp_value = enp_qnet(batch_state_tensor, intrp_action_tensor).cpu().detach().numpy().squeeze()\n",
    "                intrp_sense_value_rec.append(intrp_sense_value)\n",
    "                intrp_enp_value_rec.append(intrp_enp_value)\n",
    "                \n",
    "                # Calculate the node utility for all intrp_actions\n",
    "                final_value = pref*intrp_sense_value + (1-pref)*intrp_enp_value\n",
    "                intrp_final_value_rec.append(final_value)\n",
    "                    \n",
    "                # final_action is the action with the maximum final_value\n",
    "                final_raw_action = intrp_actions[final_value.argmax()]\n",
    "                tr_action = (final_raw_action*0.5 + 0.5)\n",
    "                \n",
    "                # Execute action\n",
    "                next_state, reward, done, _ = env.step(tr_action)\n",
    "\n",
    "            reward_rec.append(reward)\n",
    "            ep_done = done or env.RECOVERY_MODE\n",
    "            ep_done_rec.append(ep_done)\n",
    "            state = next_state\n",
    "\n",
    "        # Log the traces and summarize results\n",
    "        iteration_result={}\n",
    "\n",
    "        # Saving traces\n",
    "        iteration_result['reward_rec'] = np.array(reward_rec)\n",
    "        iteration_result['ep_done_rec'] = np.array(ep_done_rec)\n",
    "        iteration_result['action_log'] = np.array(env.action_log)\n",
    "        iteration_result['sense_dc_log'] = np.array(env.sense_dc_log)\n",
    "        iteration_result['env_log'] = np.array(env.env_log)\n",
    "        iteration_result['eno_log'] = np.array(env.eno_log)\n",
    "        iteration_result['sense_reward_log'] = np.array(env.sense_reward_log)\n",
    "        iteration_result['enp_reward_log'] = np.array(env.enp_reward_log)\n",
    "        \n",
    "        iteration_result['intrp_actions_rec'] = intrp_actions_rec\n",
    "        iteration_result['intrp_sense_value_rec'] = intrp_sense_value_rec\n",
    "        iteration_result['intrp_enp_value_rec'] = intrp_enp_value_rec\n",
    "        iteration_result['intrp_final_value_rec'] = intrp_final_value_rec\n",
    "\n",
    "        # Summarizing results\n",
    "        env_log = iteration_result['env_log']\n",
    "\n",
    "        # Get henergy metrics\n",
    "        henergy_rec = env_log[:,1]\n",
    "        avg_henergy = henergy_rec.mean()\n",
    "        iteration_result['avg_henergy'] = avg_henergy\n",
    "\n",
    "        # Get req metrics\n",
    "        req_rec = env_log[:,5]\n",
    "        avg_req = req_rec.mean()            \n",
    "        iteration_result['avg_req'] = avg_req\n",
    "\n",
    "        # Get reward metrics\n",
    "        # In this case, the reward metrics directly reflect the conformity\n",
    "        reward_rec = iteration_result['reward_rec']\n",
    "        # negative rewards = -1000 correspond to downtimes\n",
    "        # To find average reward, remove negative values\n",
    "        index = np.argwhere(reward_rec<0)\n",
    "        rwd_rec = np.delete(reward_rec, index)\n",
    "        avg_rwd = rwd_rec.mean()\n",
    "        iteration_result['avg_rwd'] = avg_rwd\n",
    "\n",
    "        # Get downtime metrics\n",
    "        ep_done_rec =  iteration_result['ep_done_rec']\n",
    "        downtimes = np.count_nonzero(ep_done_rec[:-1] < ep_done_rec[1:]) - 1 # last env done is excluded\n",
    "        iteration_result['downtimes'] = downtimes\n",
    "\n",
    "        # Get ENP metrics\n",
    "        eno_log = iteration_result['eno_log']\n",
    "        enp_log = []\n",
    "        enp_log.append(eno_log[0])\n",
    "        for t in range(1,len(eno_log)):\n",
    "            enp = enp_log[-1] + eno_log[t]\n",
    "            enp = np.clip(enp,0,1)\n",
    "            enp_log.append(enp)\n",
    "        iteration_result['enp_log'] = np.array(enp_log)\n",
    "\n",
    "#         print(\"Year:\\t\", year)            \n",
    "        exp_test_log[env_location][year] = iteration_result\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "################################################################################\n",
    "# Save Test Results\n",
    "np.save(test_log_file, exp_test_log)\n",
    "\n",
    "# In[20]:\n",
    "################################################################################\n",
    "# summarize metrics and display\n",
    "print(\"\\n\\n***TEST RESULTS****\")\n",
    "print(\"Tag:\", tag)\n",
    "print(\"Seed:\", seed)\n",
    "\n",
    "\n",
    "print(\"LOCATION\".ljust(12), \"YEAR\".ljust(6), \"HMEAN\".ljust(8), \"REQ_MEAN\".ljust(8), \"AVG_DC\".ljust(8), \n",
    "      \"SNS_RWD\".ljust(8), \"ENP_RWD\".ljust(8), \"AVG_RWD\".ljust(8), \"DOWNTIMES\".ljust(9))\n",
    "\n",
    "exp_result = exp_test_log\n",
    "location_list = list(exp_result.keys())\n",
    "for location in location_list:\n",
    "    yr_list = list(exp_result[location].keys())\n",
    "    for year in yr_list:\n",
    "        run_log = exp_result[location][year]\n",
    "        # Print summarized metrics\n",
    "        print(location.ljust(12), year, end=' ')\n",
    "        sense_avg_rwd = run_log['sense_reward_log'].mean()\n",
    "        enp_avg_rwd = run_log['enp_reward_log'].mean()\n",
    "    \n",
    "        average_rwd = run_log['avg_rwd']\n",
    "        total_downtimes = run_log['downtimes']\n",
    "        hmean = run_log['avg_henergy']\n",
    "        reqmean = run_log['avg_req']\n",
    "        sense_dc_mean = run_log['sense_dc_log'].mean()\n",
    "\n",
    "        print(f'{hmean:7.3f}',end='  ')\n",
    "        print(f'{reqmean:7.3f}',end='  ')\n",
    "        print(f'{sense_dc_mean:7.3f}',end='  ')\n",
    "        print(f'{sense_avg_rwd:7.3f}',end='  ')\n",
    "        print(f'{enp_avg_rwd:7.3f}',end='  ')\n",
    "        print(f'{average_rwd:7.3f}',end='  ')\n",
    "        print(f'{total_downtimes:5d}',end='  ')\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
