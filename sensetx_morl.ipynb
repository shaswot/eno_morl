{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# python ./pivector.py --env=cenp --gamma=0.997 --noise=0.35 --seed=123\n",
    "\n",
    "# In[1]:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40) # remove gym warning about float32 bound box precision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import common.env_lib\n",
    "from common.env_utils import task3_plot\n",
    "from common.rl_lib import (off_ReplayBuffer, \n",
    "                           ENoise, \n",
    "                           ValueNetwork,\n",
    "                           PolicyNetwork, \n",
    "                           nn_update)\n",
    "\n",
    "\n",
    "# # In[3]:\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", default=\"csense\", type=str, help=\"Environment. Specified in ./common/env_lib.py\")\n",
    "parser.add_argument(\"--gamma\", default=0.996, type=float, help=\"Discount Factor\")\n",
    "parser.add_argument(\"--noise\", default=0.8, type=float, help=\"Action Noise\")\n",
    "parser.add_argument(\"--seed\", default=238, type=int, help=\"Set seed [default: 238]\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# arguments\n",
    "seed      = args.seed\n",
    "env_name  = args.env\n",
    "GAMMA     = args.gamma\n",
    "max_noise = args.noise\n",
    "\n",
    "# seed      = 382084\n",
    "# env_name  = \"cenp\"\n",
    "# GAMMA     = 0.997\n",
    "# max_noise = 0.7\n",
    "\n",
    "intrp_no  = 4 # no. of actions to interpolate to including greedy actions\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# set seed\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "################################################################################\n",
    "# If using GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "################################################################################\n",
    "# Setting up environment base\n",
    "experiment = \"off_policy_g\" + str(GAMMA) + \"-n\" + str(max_noise) + \"-random_pref\" + \"-intrp\" + str(intrp_no)\n",
    "env = eval(\"common.env_lib.\" + env_name + \"()\")\n",
    "env = morlWrapper(env)\n",
    "\n",
    "################################################################################\n",
    "# Setup Neural Networks\n",
    "\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "hidden_dim = 256\n",
    "\n",
    "value_lr  = 1e-3\n",
    "policy_lr = 1e-4\n",
    "\n",
    "SOFT_TAU=1e-2\n",
    "\n",
    "batch_size  = 128\n",
    "replay_buffer_size = 1_000_000\n",
    "replay_buffer = off_ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "# Make model\n",
    "sense_policy_net        = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "enp_policy_net          = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "target_sense_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "target_enp_policy_net   = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "sense_value_net        = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "enp_value_net          = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "target_sense_value_net = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "target_enp_value_net   = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "sense_value_optimizer  = optim.Adam(sense_value_net.parameters(),  lr=value_lr)\n",
    "sense_policy_optimizer = optim.Adam(sense_policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "enp_value_optimizer  = optim.Adam(enp_value_net.parameters(),  lr=value_lr)\n",
    "enp_policy_optimizer = optim.Adam(enp_policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "value_criterion = nn.MSELoss()\n",
    "################################################################################\n",
    "# Setup experiment parameters\n",
    "exp_train_log ={}\n",
    "\n",
    "env_location = 'tokyo'\n",
    "exp_train_log[env_location] = {}    \n",
    "\n",
    "\n",
    "START_YEAR = 1995\n",
    "NO_OF_YEARS = 10\n",
    "timeslots_per_day = 24\n",
    "REQ_TYPE = \"random\"\n",
    "prediction_horizon = 10*timeslots_per_day\n",
    "henergy_mean= 0.13904705134356052 # 10yr hmean for tokyo\n",
    "################################################################################\n",
    "# Set up tags/folders\n",
    "\n",
    "# Tags\n",
    "env_tag = env_name + '_t' + str(timeslots_per_day) + '_' + REQ_TYPE\n",
    "model_tag = experiment +'-'+str(seed)\n",
    "\n",
    "# experiment tag\n",
    "# name of folder to save models and results\n",
    "exp_tag = env_tag  + \"-\" + experiment \n",
    "\n",
    "# experiment+seed tag\n",
    "# tensorboard tag / model filename\n",
    "tag     = exp_tag + '-' + str(seed) \n",
    "print(\"TensorBoard TAG: \",tag)\n",
    "\n",
    "# Folders\n",
    "cur_folder = os.getcwd()\n",
    "# Tensorboard Folder\n",
    "writer_folder = os.path.join(cur_folder, 'runs', exp_tag, tag )\n",
    "writer = SummaryWriter(log_dir=writer_folder)\n",
    "\n",
    "# Folder to save models\n",
    "model_folder = os.path.join(cur_folder,\"models\", exp_tag)\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder) \n",
    "\n",
    "# Folder/file to save training results\n",
    "train_results_folder = os.path.join(cur_folder,\"results\", exp_tag, \"train\")\n",
    "if not os.path.exists(train_results_folder): \n",
    "        os.makedirs(train_results_folder) \n",
    "train_log_file = os.path.join(train_results_folder, tag + '-train.npy')    \n",
    "\n",
    "# Folder/file to save test results\n",
    "test_results_folder = os.path.join(cur_folder,\"results\", exp_tag, \"test\")\n",
    "if not os.path.exists(test_results_folder): \n",
    "        os.makedirs(test_results_folder) \n",
    "test_log_file = os.path.join(test_results_folder, tag + '-test.npy')  \n",
    "\n",
    "################################################################################\n",
    "# Start setup        \n",
    "episode = 0\n",
    "frame_idx = 0\n",
    "eval_states = None # will be populated with held-out states\n",
    "\n",
    "# evaluate Q-values of random states\n",
    "NO_OF_STATES_TO_EVALUATE = timeslots_per_day*20 # how many states to sample to evaluate\n",
    "EVAL_FREQ = prediction_horizon # how often to evaluate\n",
    "\n",
    "################################################################################\n",
    "# Start training    \n",
    "for year in range(START_YEAR, START_YEAR+NO_OF_YEARS):\n",
    "    exp_noise = ENoise(env.action_space, \n",
    "                       max_sigma=max_noise, \n",
    "                       min_sigma=0.01, \n",
    "                       decay_period=30*timeslots_per_day)\n",
    "    \n",
    "    env.set_env(env_location, year, timeslots_per_day, \n",
    "                REQ_TYPE, offset=timeslots_per_day/2,\n",
    "                p_horizon=prediction_horizon,\n",
    "                hmean=henergy_mean)\n",
    "    state = env.reset()\n",
    "    reward_rec = []\n",
    "    ep_done_rec = []\n",
    "    pref_rec = []\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    counter = 0 # record of number of steps in the environment. Required to keep a finite episode length\n",
    "\n",
    "    while not done:\n",
    "        pref = np.random.random(1).item()\n",
    "        env.set_pref(pref)\n",
    "        writer.add_scalar(\"Final/Preference\", pref, frame_idx)\n",
    "        if env.RECOVERY_MODE:\n",
    "            no_action = 0            \n",
    "            next_state, reward, done, _ = env.step(no_action)\n",
    "            writer.add_scalar(\"iAction/intrp_actions[0]\", no_action, frame_idx)\n",
    "            writer.add_scalar(\"iAction/intrp_actions[1]\", no_action, frame_idx)\n",
    "            writer.add_scalar(\"iAction/intrp_actions[2]\", no_action, frame_idx)\n",
    "            writer.add_scalar(\"iAction/intrp_actions[3]\", no_action, frame_idx)\n",
    "            \n",
    "            tot_reward = env.preference*reward[0] + (1-env.preference)*reward[1]\n",
    "            reward_rec.append(tot_reward)\n",
    "            writer.add_scalar(\"Reward/sense_reward\", reward[0], frame_idx)        \n",
    "            writer.add_scalar(\"Reward/enp_reward\", reward[1], frame_idx)        \n",
    "            writer.add_scalar(\"Reward/tot_reward\", tot_reward, frame_idx)     \n",
    "\n",
    "            ep_done = done or env.RECOVERY_MODE\n",
    "            ep_done_rec.append(ep_done)\n",
    "            state = next_state\n",
    "            frame_idx +=1\n",
    "            counter = 0\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "                # sense_model# sense_model: get action and q-value\n",
    "                # convert action to DC\n",
    "                raw_sense_action_tensor = sense_policy_net(state_tensor)\n",
    "                sense_value = sense_value_net(state_tensor, raw_sense_action_tensor).cpu().item()\n",
    "                raw_sense_action = raw_sense_action_tensor.cpu().item()\n",
    "\n",
    "                # enp_model: get action and q-value\n",
    "                # convert action to DC\n",
    "                enp_action_tensor = enp_policy_net(state_tensor)\n",
    "                enp_value = enp_value_net(state_tensor, enp_action_tensor).cpu().item()\n",
    "                raw_enp_action = enp_action_tensor.cpu().item()\n",
    "\n",
    "            # Find two intermediate DC\n",
    "            intrp_actions = np.linspace(start=min(raw_sense_action,raw_enp_action), \n",
    "                                        stop=max(raw_sense_action,raw_enp_action), \n",
    "                                        num=intrp_no)\n",
    "\n",
    "            # Convert intrp_actions to action\n",
    "            intrp_action_tensor = torch.FloatTensor(intrp_actions).unsqueeze(1).to(device)\n",
    "            batch_state_tensor = torch.FloatTensor([state]*intrp_no).to(device)\n",
    "\n",
    "            # Get the q-values from sense and enp models for the intrp_actions\n",
    "            intrp_sense_value = sense_value_net(batch_state_tensor, intrp_action_tensor).cpu().detach().numpy().squeeze()\n",
    "            intrp_enp_value = enp_value_net(batch_state_tensor, intrp_action_tensor).cpu().detach().numpy().squeeze()\n",
    "\n",
    "            # Calculate the node utility for all intrp_actions\n",
    "            final_value = pref*intrp_sense_value + (1-pref)*intrp_enp_value\n",
    "\n",
    "            # final_action is the action with the maximum final_value\n",
    "            final_raw_action = intrp_actions[final_value.argmax()]\n",
    "            noisy_action = exp_noise.get_action(final_raw_action, t=0)            \n",
    "            tr_action = (noisy_action*0.5 + 0.5)\n",
    "            next_state, reward, done, _ = env.step(tr_action)\n",
    "            \n",
    "            writer.add_scalar(\"iAction/intrp_actions[0]\", intrp_actions[0], frame_idx)\n",
    "            writer.add_scalar(\"iAction/intrp_actions[1]\", intrp_actions[1], frame_idx)\n",
    "            writer.add_scalar(\"iAction/intrp_actions[2]\", intrp_actions[2], frame_idx)\n",
    "            writer.add_scalar(\"iAction/intrp_actions[3]\", intrp_actions[3], frame_idx)\n",
    "            \n",
    "            writer.add_scalar(\"iAction_Value/Q-Value[0]\", final_value[0], frame_idx)\n",
    "            writer.add_scalar(\"iAction_Value/Q-Value[1]\", final_value[1], frame_idx)\n",
    "            writer.add_scalar(\"iAction_Value/Q-Value[2]\", final_value[2], frame_idx)\n",
    "            writer.add_scalar(\"iAction_Value/Q-Value[3]\", final_value[3], frame_idx)\n",
    "            \n",
    "            writer.add_scalar(\"Final/Final Action\", final_raw_action, frame_idx)\n",
    "            writer.add_scalar(\"Final/Final Action Value\", max(final_value), frame_idx)\n",
    "            \n",
    "            tot_reward = env.preference*reward[0] + (1-env.preference)*reward[1]\n",
    "            reward_rec.append(tot_reward)\n",
    "            writer.add_scalar(\"Reward/sense_reward\", reward[0], frame_idx)        \n",
    "            writer.add_scalar(\"Reward/enp_reward\", reward[1], frame_idx)        \n",
    "            writer.add_scalar(\"Reward/tot_reward\", tot_reward, frame_idx)        \n",
    "            \n",
    "            \n",
    "            ep_done = done or env.RECOVERY_MODE or (counter==prediction_horizon-1)\n",
    "            ep_done_rec.append(ep_done)\n",
    "\n",
    "            \n",
    "            replay_buffer.push(state, final_raw_action, reward, next_state, ep_done, pref)\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                b_state, b_action, b_reward, b_next_state, b_done, b_pref = replay_buffer.sample(batch_size)\n",
    "                b_sense_reward = np.array([r[0] for r in b_reward])\n",
    "                b_enp_reward   = np.array([r[1] for r in b_reward])\n",
    "                \n",
    "                sense_batch = (b_state, b_action, b_sense_reward, b_next_state, b_done, b_pref)\n",
    "                nn_update(\"sense\", sense_batch,\n",
    "                          sense_value_net, target_sense_value_net, sense_policy_net, target_sense_policy_net,\n",
    "                          sense_value_optimizer, sense_policy_optimizer, value_criterion, device, writer, frame_idx,\n",
    "                          gamma=GAMMA,\n",
    "                          min_value=-np.inf, max_value=np.inf,\n",
    "                          soft_tau=1e-2,\n",
    "                          policy_clipgrad=0.5, value_clipgrad=0.5)\n",
    "                \n",
    "                enp_batch = (b_state, b_action, b_enp_reward, b_next_state, b_done, 1-b_pref)\n",
    "                nn_update(\"enp\", enp_batch,\n",
    "                          enp_value_net, target_enp_value_net, enp_policy_net, target_enp_policy_net,\n",
    "                          enp_value_optimizer, enp_policy_optimizer, value_criterion, device, writer, frame_idx,\n",
    "                          gamma=GAMMA,\n",
    "                          min_value=-np.inf, max_value=np.inf,\n",
    "                          soft_tau=1e-2,\n",
    "                          policy_clipgrad=0.5, value_clipgrad=0.5)\n",
    "                    \n",
    "            episode_reward += tot_reward\n",
    "            frame_idx +=1\n",
    "            counter +=1\n",
    "            state = next_state\n",
    "            \n",
    "            if ep_done:\n",
    "                writer.add_scalar(\"Episode_reward\", episode_reward, episode)\n",
    "                episode_reward = 0 # reset episode reward\n",
    "                episode +=1 # increase episode count\n",
    "                counter = 0 # reset episode counter\n",
    "\n",
    "        \n",
    "#         if len(replay_buffer) > 3*NO_OF_STATES_TO_EVALUATE and eval_states is None:\n",
    "#             eval_states, eval_actions, _ ,_ , _ = replay_buffer.sample(NO_OF_STATES_TO_EVALUATE)\n",
    "#             eval_states = torch.FloatTensor(eval_states).to(device)\n",
    "#             eval_actions = torch.FloatTensor(eval_actions).unsqueeze(1).to(device)\n",
    "\n",
    "#         if eval_states is not None and frame_idx % EVAL_FREQ == 0:\n",
    "#             mean_val = calc_values_of_states(eval_states, eval_actions, value_net, device=device)\n",
    "#             writer.add_scalar(\"values_mean\", mean_val, frame_idx)\n",
    "                \n",
    "     # Log the traces and summarize results\n",
    "    iteration_result={}\n",
    "\n",
    "    # Saving traces\n",
    "    iteration_result['reward_rec'] = np.array(reward_rec)\n",
    "    iteration_result['ep_done_rec'] = np.array(ep_done_rec)\n",
    "    iteration_result['action_log'] = np.array(env.action_log)\n",
    "    iteration_result['sense_dc_log'] = np.array(env.sense_dc_log)\n",
    "    iteration_result['env_log'] = np.array(env.env_log)\n",
    "    iteration_result['eno_log'] = np.array(env.eno_log)\n",
    "    iteration_result['sense_reward_log'] = np.array(env.sense_reward_log)\n",
    "    iteration_result['enp_reward_log'] = np.array(env.enp_reward_log)\n",
    "\n",
    "    \n",
    "    # Summarizing results\n",
    "    env_log = iteration_result['env_log']\n",
    "\n",
    "    # Get henergy metrics\n",
    "    henergy_rec = env_log[:,1]\n",
    "    avg_henergy = henergy_rec.mean()\n",
    "    iteration_result['avg_henergy'] = avg_henergy\n",
    "\n",
    "    # Get req metrics\n",
    "    req_rec = env_log[:,5]\n",
    "    avg_req = req_rec.mean()            \n",
    "    iteration_result['avg_req'] = avg_req\n",
    "\n",
    "    # Get reward metrics\n",
    "    # In this case, the reward metrics directly reflect the conformity\n",
    "    reward_recx = iteration_result['reward_rec']\n",
    "    # negative rewards = -1000 correspond to downtimes\n",
    "    # To find average reward, remove negative values\n",
    "    index = np.argwhere(reward_recx<0)\n",
    "    rwd_recx = np.delete(reward_recx, index)\n",
    "    avg_rwd = rwd_recx.mean()\n",
    "    iteration_result['avg_rwd'] = avg_rwd\n",
    "\n",
    "    # Get downtime metrics\n",
    "    batt_rec = env_log[:,3]\n",
    "    batt_rec[batt_rec>0.1]=0\n",
    "    batt_rec[batt_rec!=0]=1\n",
    "    downtimes = np.count_nonzero(batt_rec[:-1] < batt_rec[1:])\n",
    "    iteration_result['downtimes'] = downtimes\n",
    "#     # Print summary\n",
    "#     print(\"Year:\\t\", year)            \n",
    "#     print(\"Avg. Supply: \\t\", avg_henergy)\n",
    "#     print(\"Avg. Request: \\t\", avg_req)\n",
    "#     print(\"Avg reward:\\t\", avg_rwd)\n",
    "#     print(\"Downtimes:\\t\", downtimes)\n",
    "    \n",
    "#     # Plot traces\n",
    "#     sorl_plot(iteration_result, timeslots_per_day, START_DAY=0, NO_OF_DAY_TO_PLOT = 363)\n",
    "    \n",
    "    # Store results, traces and summary\n",
    "    exp_train_log[env_location][year] = iteration_result\n",
    "#     print(\"*\"*70)\n",
    "#     print(\"\")\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "################################################################################\n",
    "# Save Training Results in file\n",
    "np.save(train_log_file, exp_train_log)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "################################################################################\n",
    "# display training performance metrics\n",
    "train_log = exp_train_log[env_location]\n",
    "print(\"\\n\\n***TRAINING PERFORMANCE****\")\n",
    "print(\"YEAR\".ljust(6), \"HMEAN\".ljust(8), \"REQ_MEAN\".ljust(8), \"AVG_DC\".ljust(8), \n",
    "      \"SNS_RWD\".ljust(8), \"ENP_RWD\".ljust(8), \"AVG_RWD\".ljust(8), \"DOWNTIMES\".ljust(9))\n",
    "for year in list(train_log.keys()):\n",
    "    iteration_result =  train_log[year]\n",
    "    \n",
    "    # Print summarized metrics\n",
    "    print(year,end=' ')\n",
    "    sense_avg_rwd = iteration_result['sense_reward_log'].mean()\n",
    "    enp_avg_rwd = iteration_result['enp_reward_log'].mean()\n",
    "    average_rwd = iteration_result['avg_rwd']\n",
    "    total_downtimes = iteration_result['downtimes']\n",
    "    hmean = iteration_result['avg_henergy']\n",
    "    reqmean = iteration_result['avg_req']\n",
    "    sense_dc_mean = iteration_result['sense_dc_log'].mean()\n",
    "    \n",
    "    print(f'{hmean:7.3f}',end='  ')\n",
    "    print(f'{reqmean:7.3f}',end='  ')\n",
    "    print(f'{sense_dc_mean:7.3f}',end='  ')\n",
    "    print(f'{sense_avg_rwd:7.3f}',end='  ')\n",
    "    print(f'{enp_avg_rwd:7.3f}',end='  ')\n",
    "    print(f'{average_rwd:7.3f}',end='  ')\n",
    "    print(f'{total_downtimes:9d}',end='  ')\n",
    "    print(\"\")\n",
    "\n",
    "# In[14]:\n",
    "################################################################################\n",
    "# Save policy model weights with filename as tag\n",
    "sense_policy_net_model_file = os.path.join(model_folder, (tag + \"-sense_policy.pt\"))\n",
    "torch.save(sense_policy_net.state_dict(), sense_policy_net_model_file)\n",
    "\n",
    "enp_policy_net_model_file = os.path.join(model_folder, (tag + \"-enp_policy.pt\"))\n",
    "torch.save(enp_policy_net.state_dict(), enp_policy_net_model_file)\n",
    "# In[15]:\n",
    "\n",
    "################################################################################\n",
    "# Save value model weights with filename as tag\n",
    "sense_value_net_model_file = os.path.join(model_folder, (tag + \"-sense_value.pt\"))\n",
    "torch.save(sense_value_net.state_dict(), sense_value_net_model_file)\n",
    "\n",
    "enp_value_net_model_file = os.path.join(model_folder, (tag + \"-enp_value.pt\"))\n",
    "torch.save(enp_value_net.state_dict(), enp_value_net_model_file)\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In[16]:\n",
    "# # Testing\n",
    "# test_pref_list = [0.2,0.5,0.8]\n",
    "# # Make model\n",
    "\n",
    "# sense_pnet = sense_policy_net\n",
    "# enp_pnet   = enp_policy_net\n",
    "\n",
    "# sense_qnet = sense_value_net\n",
    "# enp_qnet   = enp_value_net\n",
    "\n",
    "\n",
    "# sense_pnet.eval()\n",
    "# sense_qnet.eval()\n",
    "# enp_pnet.eval()\n",
    "# enp_qnet.eval();\n",
    "\n",
    "# env_location_list = ['tokyo']\n",
    "# START_YEAR  = 1995\n",
    "# NO_OF_YEARS = 24\n",
    "\n",
    "# exp_test_log = {} # dictionary to store test results\n",
    "# for env_location in env_location_list:\n",
    "# #     print(env_location)\n",
    "#     exp_test_log[env_location] = {}\n",
    "#     for year in range(START_YEAR, START_YEAR+NO_OF_YEARS):\n",
    "#         exp_test_log[env_location][year]={}\n",
    "#         for pref in test_pref_list:\n",
    "# #             print(\"\\n\\nPreference: \", pref)\n",
    "#             exp_test_log[env_location][year][pref]={}\n",
    "#             env.set_pref(pref)\n",
    "#             env.set_env(env_location,year, timeslots_per_day, \n",
    "#                         REQ_TYPE, offset=timeslots_per_day/2,\n",
    "#                         p_horizon=prediction_horizon,\n",
    "#                         hmean=henergy_mean)    \n",
    "#             state = env.reset()\n",
    "#             reward_rec = []\n",
    "\n",
    "#             intrp_actions_rec = []\n",
    "#             intrp_sense_value_rec = []\n",
    "#             intrp_enp_value_rec = []\n",
    "#             intrp_final_value_rec = []\n",
    "\n",
    "#             ep_done_rec = []\n",
    "#             done = False\n",
    "#             while not done:\n",
    "#                 if env.RECOVERY_MODE:\n",
    "#                     no_action = 0            \n",
    "#                     next_state, reward, done, _ = env.step(no_action)       \n",
    "#                 else:\n",
    "#                     with torch.no_grad():\n",
    "#                         state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "#                         # sense_model# sense_model: get action and q-value\n",
    "#                         # convert action to DC\n",
    "#                         raw_sense_action_tensor = sense_pnet(state_tensor)\n",
    "#                         sense_value = sense_qnet(state_tensor, raw_sense_action_tensor).cpu().item()\n",
    "#                         raw_sense_action = raw_sense_action_tensor.cpu().item()\n",
    "\n",
    "#                         # enp_model: get action and q-value\n",
    "#                         # convert action to DC\n",
    "#                         enp_action_tensor = enp_pnet(state_tensor)\n",
    "#                         enp_value = enp_qnet(state_tensor, enp_action_tensor).cpu().item()\n",
    "#                         raw_enp_action = enp_action_tensor.cpu().item()\n",
    "\n",
    "#                     # Find two intermediate DC\n",
    "#                     intrp_actions = np.linspace(start=min(raw_sense_action,raw_enp_action), \n",
    "#                                                 stop=max(raw_sense_action,raw_enp_action), \n",
    "#                                                 num=intrp_no)\n",
    "#                     intrp_actions_rec.append(intrp_actions)\n",
    "\n",
    "#                     # Convert intrp_actions to action\n",
    "#                     intrp_action_tensor = torch.FloatTensor(intrp_actions).unsqueeze(1).to(device)\n",
    "#                     batch_state_tensor = torch.FloatTensor([state]*intrp_no).to(device)\n",
    "\n",
    "#                     # Get the q-values from sense and enp models for the intrp_actions\n",
    "#                     intrp_sense_value = sense_qnet(batch_state_tensor, intrp_action_tensor).cpu().detach().numpy().squeeze()\n",
    "#                     intrp_enp_value = enp_qnet(batch_state_tensor, intrp_action_tensor).cpu().detach().numpy().squeeze()\n",
    "#                     intrp_sense_value_rec.append(intrp_sense_value)\n",
    "#                     intrp_enp_value_rec.append(intrp_enp_value)\n",
    "                \n",
    "#                     # Calculate the node utility for all intrp_actions\n",
    "#                     final_value = pref*intrp_sense_value + (1-pref)*intrp_enp_value\n",
    "#                     intrp_final_value_rec.append(final_value)\n",
    "                    \n",
    "#                     # final_action is the action with the maximum final_value\n",
    "#                     final_raw_action = intrp_actions[final_value.argmax()]\n",
    "#                     tr_action = (final_raw_action*0.5 + 0.5)\n",
    "                \n",
    "#                     # Execute action\n",
    "#                     next_state, reward, done, _ = env.step(tr_action)\n",
    "#                 tot_reward = env.preference*reward[0] + (1-env.preference)*reward[1]\n",
    "#                 reward_rec.append(tot_reward)\n",
    "#                 ep_done = done or env.RECOVERY_MODE\n",
    "#                 ep_done_rec.append(ep_done)\n",
    "#                 state = next_state\n",
    "\n",
    "#             # Log the traces and summarize results\n",
    "#             iteration_result={}\n",
    "\n",
    "#             # Saving traces\n",
    "#             iteration_result['reward_rec'] = np.array(reward_rec)\n",
    "#             iteration_result['ep_done_rec'] = np.array(ep_done_rec)\n",
    "#             iteration_result['action_log'] = np.array(env.action_log)\n",
    "#             iteration_result['sense_dc_log'] = np.array(env.sense_dc_log)\n",
    "#             iteration_result['env_log'] = np.array(env.env_log)\n",
    "#             iteration_result['eno_log'] = np.array(env.eno_log)\n",
    "#             iteration_result['sense_reward_log'] = np.array(env.sense_reward_log)\n",
    "#             iteration_result['enp_reward_log'] = np.array(env.enp_reward_log)\n",
    "\n",
    "#             iteration_result['intrp_dc_rec'] = intrp_dc_rec\n",
    "#             iteration_result['intrp_sense_value_rec'] = intrp_sense_value_rec\n",
    "#             iteration_result['intrp_enp_value_rec'] = intrp_enp_value_rec\n",
    "#             iteration_result['intrp_final_value_rec'] = intrp_final_value_rec\n",
    "\n",
    "#             # Summarizing results\n",
    "#             env_log = iteration_result['env_log']\n",
    "\n",
    "#             # Get henergy metrics\n",
    "#             henergy_rec = env_log[:,1]\n",
    "#             avg_henergy = henergy_rec.mean()\n",
    "#             iteration_result['avg_henergy'] = avg_henergy\n",
    "\n",
    "#             # Get req metrics\n",
    "#             req_rec = env_log[:,5]\n",
    "#             avg_req = req_rec.mean()            \n",
    "#             iteration_result['avg_req'] = avg_req\n",
    "\n",
    "#             # Get reward metrics\n",
    "#             # In this case, the reward metrics directly reflect the conformity\n",
    "#             reward_rec = iteration_result['reward_rec']\n",
    "#             # negative rewards = -1000 correspond to downtimes\n",
    "#             # To find average reward, remove negative values\n",
    "#             index = np.argwhere(reward_rec<0)\n",
    "#             rwd_rec = np.delete(reward_rec, index)\n",
    "#             avg_rwd = rwd_rec.mean()\n",
    "#             iteration_result['avg_rwd'] = avg_rwd\n",
    "\n",
    "#             # Get downtime metrics\n",
    "#             ep_done_rec =  iteration_result['ep_done_rec']\n",
    "#             downtimes = np.count_nonzero(ep_done_rec[:-1] < ep_done_rec[1:]) - 1 # last env done is excluded\n",
    "#             iteration_result['downtimes'] = downtimes\n",
    "\n",
    "#     #         print(\"Year:\\t\", year)            \n",
    "#             exp_test_log[env_location][year][pref] = iteration_result\n",
    "# #     print(\"\")\n",
    "\n",
    "# ################################################################################\n",
    "# # Save Test Results\n",
    "# np.save(test_log_file, exp_test_log)\n",
    "\n",
    "# # summarize metrics and display\n",
    "# print(\"\\n\\n***TEST RESULTS****\")\n",
    "# print(\"Tag:\", tag)\n",
    "# print(\"Seed:\", seed)\n",
    "\n",
    "# for pref in test_pref_list:\n",
    "#     print(\"\\n\\nPreference: \", pref)\n",
    "#     print(\"LOCATION\".ljust(12), \"YEAR\".ljust(6), \"HMEAN\".ljust(8), \"REQ_MEAN\".ljust(8), \"AVG_DC\".ljust(8), \n",
    "#           \"SNS_RWD\".ljust(8), \"ENP_RWD\".ljust(8), \"AVG_RWD\".ljust(8), \"DOWNTIMES\".ljust(9))\n",
    "    \n",
    "#     exp_result = exp_test_log\n",
    "#     location_list = list(exp_result.keys())\n",
    "#     for location in location_list:\n",
    "#         yr_list = list(exp_result[location].keys())\n",
    "#         for year in yr_list:\n",
    "#             run_log = exp_result[location][year][pref]\n",
    "#             # Print summarized metrics\n",
    "#             print(location.ljust(12), year, end=' ')\n",
    "#             sense_avg_rwd = run_log['sense_reward_log'].mean()\n",
    "#             enp_avg_rwd = run_log['enp_reward_log'].mean()\n",
    "\n",
    "#             average_rwd = run_log['avg_rwd']\n",
    "#             total_downtimes = run_log['downtimes']\n",
    "#             hmean = run_log['avg_henergy']\n",
    "#             reqmean = run_log['avg_req']\n",
    "#             sense_dc_mean = run_log['sense_dc_log'].mean()\n",
    "\n",
    "#             print(f'{hmean:7.3f}',end='  ')\n",
    "#             print(f'{reqmean:7.3f}',end='  ')\n",
    "#             print(f'{sense_dc_mean:7.3f}',end='  ')\n",
    "#             print(f'{sense_avg_rwd:7.3f}',end='  ')\n",
    "#             print(f'{enp_avg_rwd:7.3f}',end='  ')\n",
    "#             print(f'{average_rwd:7.3f}',end='  ')\n",
    "#             print(f'{total_downtimes:5d}',end='  ')\n",
    "#             print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
