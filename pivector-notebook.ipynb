{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard TAG:  cenp_t24_random-pimorl_g0.997-n0.7-753\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# python ./pivector.py --env=cenp --gamma=0.997 --noise=0.7 --seed=123\n",
    "\n",
    "# In[1]:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40) # remove gym warning about float32 bound box precision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import common.env_lib\n",
    "from common.env_utils import sorl_plot\n",
    "from common.rl_lib import ReplayBuffer, OUNoise, ENoise, ValueNetwork, PolicyNetwork, ddpg_update, calc_values_of_states\n",
    "\n",
    "\n",
    "# # In[3]:\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--env\", default=\"csense\", type=str, help=\"Environment. Specified in ./common/env_lib.py\")\n",
    "# parser.add_argument(\"--gamma\", default=0.996, type=float, help=\"Discount Factor\")\n",
    "# parser.add_argument(\"--noise\", default=0.8, type=float, help=\"Action Noise\")\n",
    "# parser.add_argument(\"--seed\", default=238, type=int, help=\"Set seed [default: 238]\")\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# # arguments\n",
    "# seed      = args.seed\n",
    "# env_name  = args.env\n",
    "# GAMMA     = args.gamma\n",
    "# max_noise = args.noise\n",
    "# pref      = args.pref\n",
    "\n",
    "seed      = 753\n",
    "env_name  = \"cenp\"\n",
    "GAMMA     = 0.997\n",
    "max_noise = 0.7\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# set seed\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "################################################################################\n",
    "# If using GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "################################################################################\n",
    "# Reward is now a tuple, not a scalar\n",
    "class morlWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(morlWrapper, self).__init__(env)\n",
    "    \n",
    "    def reward(self,action): # reward based on utility\n",
    "            if action < 0:\n",
    "                sense_reward = 0\n",
    "                enp_reward = 0\n",
    "                reward = 0\n",
    "\n",
    "            else:\n",
    "                sense_dc = self.action2sensedc(action)\n",
    "                sense_reward = min(1.0,sense_dc/self.req_obs)\n",
    "\n",
    "                batt_threshold = 0.8\n",
    "                if self.menergy_obs > batt_threshold:\n",
    "                    enp_reward = 1\n",
    "                else:\n",
    "                    enp_reward = (self.menergy_obs - self.MIN_BATT)/(batt_threshold-self.MIN_BATT)\n",
    "\n",
    "            self.sense_reward_log.append(sense_reward)\n",
    "            self.enp_reward_log.append(enp_reward)\n",
    "            return (sense_reward, enp_reward)\n",
    "################################################################################\n",
    "# Setting up environment base\n",
    "experiment = \"pimorl_g\" + str(GAMMA) + \"-n\" + str(max_noise)\n",
    "env = eval(\"common.env_lib.\" + env_name + \"()\")\n",
    "env = morlWrapper(env)\n",
    "\n",
    "################################################################################\n",
    "# Setup Neural Networks\n",
    "\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "hidden_dim = 256\n",
    "\n",
    "value_lr  = 1e-3\n",
    "policy_lr = 1e-4\n",
    "\n",
    "SOFT_TAU=1e-2\n",
    "\n",
    "batch_size  = 128\n",
    "replay_buffer_size = 1000000\n",
    "sense_replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "enp_replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "# Make model\n",
    "sense_policy_net        = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "enp_policy_net          = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "target_sense_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "target_enp_policy_net   = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "sense_value_net        = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "enp_value_net          = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "target_sense_value_net = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "target_enp_value_net   = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "sense_value_optimizer  = optim.Adam(sense_value_net.parameters(),  lr=value_lr)\n",
    "sense_policy_optimizer = optim.Adam(sense_policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "enp_value_optimizer  = optim.Adam(enp_value_net.parameters(),  lr=value_lr)\n",
    "enp_policy_optimizer = optim.Adam(enp_policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "value_criterion = nn.MSELoss()\n",
    "################################################################################\n",
    "# Setup experiment parameters\n",
    "exp_train_log ={}\n",
    "\n",
    "env_location = 'tokyo'\n",
    "exp_train_log[env_location] = {}    \n",
    "\n",
    "\n",
    "START_YEAR = 1995\n",
    "NO_OF_YEARS = 1\n",
    "timeslots_per_day = 24\n",
    "REQ_TYPE = \"random\"\n",
    "prediction_horizon = 10*timeslots_per_day\n",
    "henergy_mean= 0.13904705134356052 # 10yr hmean for tokyo\n",
    "################################################################################\n",
    "# Set up tags/folders\n",
    "\n",
    "# Tags\n",
    "env_tag = env_name + '_t' + str(timeslots_per_day) + '_' + REQ_TYPE\n",
    "model_tag = experiment +'-'+str(seed)\n",
    "\n",
    "# experiment tag\n",
    "# name of folder to save models and results\n",
    "exp_tag = env_tag  + \"-\" + experiment \n",
    "\n",
    "# experiment+seed tag\n",
    "# tensorboard tag / model filename\n",
    "tag     = exp_tag + '-' + str(seed) \n",
    "print(\"TensorBoard TAG: \",tag)\n",
    "\n",
    "# Folders\n",
    "cur_folder = os.getcwd()\n",
    "# Tensorboard Folder\n",
    "writer_folder = os.path.join(cur_folder, 'runs', exp_tag, tag )\n",
    "writer = SummaryWriter(log_dir=writer_folder)\n",
    "\n",
    "# Folder to save models\n",
    "model_folder = os.path.join(cur_folder,\"models\", exp_tag)\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder) \n",
    "\n",
    "# Folder/file to save training results\n",
    "train_results_folder = os.path.join(cur_folder,\"results\", exp_tag, \"train\")\n",
    "if not os.path.exists(train_results_folder): \n",
    "        os.makedirs(train_results_folder) \n",
    "train_log_file = os.path.join(train_results_folder, tag + '-train.npy')    \n",
    "\n",
    "# Folder/file to save test results\n",
    "test_results_folder = os.path.join(cur_folder,\"results\", exp_tag, \"test\")\n",
    "if not os.path.exists(test_results_folder): \n",
    "        os.makedirs(test_results_folder) \n",
    "test_log_file = os.path.join(test_results_folder, tag + '-test.npy')  \n",
    "\n",
    "################################################################################\n",
    "# Start setup        \n",
    "episode = 0\n",
    "frame_idx = 0\n",
    "eval_states = None # will be populated with held-out states\n",
    "\n",
    "# evaluate Q-values of random states\n",
    "NO_OF_STATES_TO_EVALUATE = timeslots_per_day*20 # how many states to sample to evaluate\n",
    "EVAL_FREQ = prediction_horizon # how often to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Start training\n",
    "for year in range(START_YEAR, START_YEAR+NO_OF_YEARS):\n",
    "       \n",
    "    exp_noise = ENoise(env.action_space, \n",
    "                       max_sigma=max_noise, \n",
    "                       min_sigma=0.01, \n",
    "                       decay_period=30*timeslots_per_day)\n",
    "    \n",
    "    env.set_env(env_location, year, timeslots_per_day, \n",
    "                REQ_TYPE, offset=timeslots_per_day/2,\n",
    "                p_horizon=prediction_horizon,\n",
    "                hmean=henergy_mean)\n",
    "    state = env.reset()\n",
    "    reward_rec = []\n",
    "    ep_done_rec = []\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "    done = False\n",
    "    counter = 0 # record of number of steps in the environment. Required to keep a finite episode length\n",
    "\n",
    "    while not done:\n",
    "        if env.RECOVERY_MODE:\n",
    "            no_action = 0            \n",
    "            next_state, reward, done, _ = env.step(no_action)\n",
    "            writer.add_scalar(\"Action/0_raw_action\", no_action, frame_idx)\n",
    "            writer.add_scalar(\"Action/1_noisy_action\", no_action, frame_idx)\n",
    "            writer.add_scalar(\"Action/2_tr_action\", no_action, frame_idx)\n",
    "            writer.add_scalar(\"Reward/reward\", reward, frame_idx)        \n",
    "            reward_rec.append(reward)\n",
    "            ep_done = done or env.RECOVERY_MODE\n",
    "            ep_done_rec.append(ep_done)\n",
    "            state = next_state\n",
    "            frame_idx +=1\n",
    "            counter = 0\n",
    "\n",
    "        else:\n",
    "            raw_action = policy_net.get_action(state)\n",
    "            noisy_action = exp_noise.get_action(raw_action, t=0)\n",
    "            tr_action = (noisy_action*0.5 + 0.5)\n",
    "            next_state, reward, done, _ = env.step(tr_action) \n",
    "            writer.add_scalar(\"Action/0_raw_action\", raw_action, frame_idx)\n",
    "            writer.add_scalar(\"Action/1_noisy_action\", noisy_action, frame_idx)\n",
    "            writer.add_scalar(\"Action/2_tr_action\", tr_action, frame_idx)\n",
    "            writer.add_scalar(\"Reward/reward\", reward, frame_idx)        \n",
    "            reward_rec.append(reward)\n",
    "            ep_done = done or env.RECOVERY_MODE or (counter==prediction_horizon-1)\n",
    "            ep_done_rec.append(ep_done)\n",
    "        \n",
    "            replay_buffer.push(state, noisy_action, reward, next_state, ep_done)\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                ddpg_update(value_net,\n",
    "                            target_value_net,\n",
    "                            policy_net,\n",
    "                            target_policy_net,\n",
    "                            value_optimizer,\n",
    "                            policy_optimizer,\n",
    "                            value_criterion,\n",
    "                            batch_size,\n",
    "                            replay_buffer,\n",
    "                            device,\n",
    "                            writer,\n",
    "                            frame_idx,\n",
    "                            gamma=GAMMA,\n",
    "                            min_value=-np.inf,\n",
    "                            max_value=np.inf,\n",
    "                            soft_tau=1e-2,\n",
    "                            policy_clipgrad=0.5,\n",
    "                            value_clipgrad=0.5)\n",
    "            episode_reward += reward\n",
    "            step += 1 # to decrease exploration noise\n",
    "            frame_idx +=1\n",
    "            counter +=1\n",
    "            state = next_state\n",
    "            \n",
    "            if ep_done:\n",
    "                writer.add_scalar(\"Reward/episode_reward\", episode_reward, episode)\n",
    "                episode_reward = 0 # reset episode reward\n",
    "                episode +=1 # increase episode count\n",
    "                counter = 0 # reset episode counter\n",
    "                # as episode count grows, start with lesser exploration noise\n",
    "                step = episode \n",
    "            if env.RECOVERY_MODE:\n",
    "                # start with increased exploration noise if recovery mode\n",
    "                step -= timeslots_per_day\n",
    "                step = max(0,step)\n",
    "        \n",
    "        if len(replay_buffer) > 3*NO_OF_STATES_TO_EVALUATE and eval_states is None:\n",
    "            eval_states, eval_actions, _ ,_ , _ = replay_buffer.sample(NO_OF_STATES_TO_EVALUATE)\n",
    "            eval_states = torch.FloatTensor(eval_states).to(device)\n",
    "            eval_actions = torch.FloatTensor(eval_actions).unsqueeze(1).to(device)\n",
    "\n",
    "        if eval_states is not None and frame_idx % EVAL_FREQ == 0:\n",
    "            mean_val = calc_values_of_states(eval_states, eval_actions, value_net, device=device)\n",
    "            writer.add_scalar(\"values_mean\", mean_val, frame_idx)\n",
    "                \n",
    "     # Log the traces and summarize results\n",
    "    iteration_result={}\n",
    "\n",
    "    # Saving traces\n",
    "    iteration_result['reward_rec'] = np.array(reward_rec)\n",
    "    iteration_result['ep_done_rec'] = np.array(ep_done_rec)\n",
    "    iteration_result['action_log'] = np.array(env.action_log)\n",
    "    iteration_result['sense_dc_log'] = np.array(env.sense_dc_log)\n",
    "    iteration_result['env_log'] = np.array(env.env_log)\n",
    "    iteration_result['eno_log'] = np.array(env.eno_log)\n",
    "    iteration_result['sense_reward_log'] = np.array(env.sense_reward_log)\n",
    "    iteration_result['enp_reward_log'] = np.array(env.enp_reward_log)\n",
    "\n",
    "    \n",
    "    # Summarizing results\n",
    "    env_log = iteration_result['env_log']\n",
    "\n",
    "    # Get henergy metrics\n",
    "    henergy_rec = env_log[:,1]\n",
    "    avg_henergy = henergy_rec.mean()\n",
    "    iteration_result['avg_henergy'] = avg_henergy\n",
    "\n",
    "    # Get req metrics\n",
    "    req_rec = env_log[:,5]\n",
    "    avg_req = req_rec.mean()            \n",
    "    iteration_result['avg_req'] = avg_req\n",
    "\n",
    "    # Get reward metrics\n",
    "    # In this case, the reward metrics directly reflect the conformity\n",
    "    reward_recx = iteration_result['reward_rec']\n",
    "    # negative rewards = -1000 correspond to downtimes\n",
    "    # To find average reward, remove negative values\n",
    "    index = np.argwhere(reward_recx<0)\n",
    "    rwd_recx = np.delete(reward_recx, index)\n",
    "    avg_rwd = rwd_recx.mean()\n",
    "    iteration_result['avg_rwd'] = avg_rwd\n",
    "\n",
    "    # Get downtime metrics\n",
    "    batt_rec = env_log[:,3]\n",
    "    batt_rec[batt_rec>0.1]=0\n",
    "    batt_rec[batt_rec!=0]=1\n",
    "    downtimes = np.count_nonzero(batt_rec[:-1] < batt_rec[1:])\n",
    "    iteration_result['downtimes'] = downtimes\n",
    "\n",
    "    # Get ENP metrics\n",
    "    eno_log = iteration_result['eno_log']\n",
    "    enp_log = []\n",
    "    enp_log.append(eno_log[0])\n",
    "    for t in range(1,len(eno_log)):\n",
    "        enp = enp_log[-1] + eno_log[t]\n",
    "        enp = np.clip(enp,0,1)\n",
    "        enp_log.append(enp)\n",
    "    iteration_result['enp_log'] = np.array(enp_log)\n",
    "    \n",
    "#     # Print summary\n",
    "#     print(\"Year:\\t\", year)            \n",
    "#     print(\"Avg. Supply: \\t\", avg_henergy)\n",
    "#     print(\"Avg. Request: \\t\", avg_req)\n",
    "#     print(\"Avg reward:\\t\", avg_rwd)\n",
    "#     print(\"Downtimes:\\t\", downtimes)\n",
    "    \n",
    "#     # Plot traces\n",
    "#     sorl_plot(iteration_result, timeslots_per_day, START_DAY=0, NO_OF_DAY_TO_PLOT = 363)\n",
    "    \n",
    "    # Store results, traces and summary\n",
    "    exp_train_log[env_location][year] = iteration_result\n",
    "#     print(\"*\"*70)\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "################################################################################\n",
    "# Save Training Results in file\n",
    "np.save(train_log_file, exp_train_log)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "################################################################################\n",
    "# display training performance metrics\n",
    "train_log = exp_train_log[env_location]\n",
    "print(\"\\n\\n***TRAINING PERFORMANCE****\")\n",
    "print(\"YEAR\".ljust(6), \"HMEAN\".ljust(8), \"REQ_MEAN\".ljust(8), \"AVG_DC\".ljust(8), \n",
    "      \"SNS_RWD\".ljust(8), \"ENP_RWD\".ljust(8), \"AVG_RWD\".ljust(8), \"DOWNTIMES\".ljust(9))\n",
    "for year in list(train_log.keys()):\n",
    "    iteration_result =  train_log[year]\n",
    "    # Print summarized metrics\n",
    "    print(year, end=' ')\n",
    "    sense_avg_rwd = iteration_result['sense_reward_log'].mean()\n",
    "    enp_avg_rwd = iteration_result['enp_reward_log'].mean()\n",
    "    average_rwd = iteration_result['avg_rwd']\n",
    "    total_downtimes = iteration_result['downtimes']\n",
    "    hmean = iteration_result['avg_henergy']\n",
    "    reqmean = iteration_result['avg_req']\n",
    "    sense_dc_mean = iteration_result['sense_dc_log'].mean()\n",
    "    \n",
    "    print(f'{hmean:7.3f}',end='  ')\n",
    "    print(f'{reqmean:7.3f}',end='  ')\n",
    "    print(f'{sense_dc_mean:7.3f}',end='  ')\n",
    "    print(f'{sense_avg_rwd:7.3f}',end='  ')\n",
    "    print(f'{enp_avg_rwd:7.3f}',end='  ')\n",
    "    print(f'{average_rwd:7.3f}',end='  ')\n",
    "    print(f'{total_downtimes:5d}',end='  ')\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "################################################################################\n",
    "# Save policy model weights with filename as tag\n",
    "model_file = os.path.join(model_folder, (tag + \"-policy.pt\"))\n",
    "torch.save(policy_net.state_dict(), model_file)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "################################################################################\n",
    "# Save value model weights with filename as tag\n",
    "model_file = os.path.join(model_folder, (tag + \"-value.pt\"))\n",
    "torch.save(value_net.state_dict(), model_file)\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "################################################################################\n",
    "# Testing out model\n",
    "\n",
    "# print(\"Experiment:\", exp_tag)\n",
    "# print(\"Seed:\", seed)\n",
    "\n",
    "# Make environment\n",
    "env = eval(\"common.env_lib.\"+env_name+\"()\")\n",
    "env = non_markov_Wrapper(env)\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# Make model\n",
    "ddpg_net = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "# Load model weights\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\", exp_tag)\n",
    "model_file   = os.path.join(model_folder, (tag + \"-policy.pt\"))\n",
    "ddpg_net.load_state_dict(torch.load(model_file))\n",
    "ddpg_net.eval()\n",
    "    \n",
    "\n",
    "# Setup environment\n",
    "env_location_list = ['tokyo']#['tokyo','wakkanai','minamidaito']\n",
    "START_YEAR = 1995\n",
    "NO_OF_YEARS = 24\n",
    "# timeslots_per_day = 24\n",
    "# REQ_TYPE = \"random\"\n",
    "# prediction_horizon = 10*timeslots_per_day\n",
    "# henergy_mean= 0.13904705134356052 # 10yr hmean for tokyo\n",
    "\n",
    "exp_test_log = {}\n",
    "\n",
    "for env_location in env_location_list:\n",
    "#     print(env_location)\n",
    "    exp_test_log[env_location] = {}\n",
    "    for year in range(START_YEAR, START_YEAR+NO_OF_YEARS):\n",
    "        exp_test_log[env_location][year]={}\n",
    "        \n",
    "        env.set_env(env_location,year, timeslots_per_day, \n",
    "                    REQ_TYPE, offset=timeslots_per_day/2,\n",
    "                    p_horizon=prediction_horizon,\n",
    "                    hmean=henergy_mean)    \n",
    "        state = env.reset()\n",
    "        reward_rec = []\n",
    "        ep_done_rec = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.RECOVERY_MODE:\n",
    "                no_action = 0            \n",
    "                next_state, reward, done, _ = env.step(no_action)       \n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    raw_action = ddpg_net.get_action(state)\n",
    "                noisy_action = raw_action\n",
    "                tr_action = (noisy_action*0.5 + 0.5)\n",
    "                next_state, reward, done, _ = env.step(tr_action)\n",
    "            reward_rec.append(reward)\n",
    "            ep_done = done or env.RECOVERY_MODE\n",
    "            ep_done_rec.append(ep_done)\n",
    "            state = next_state\n",
    "\n",
    "        # Log the traces and summarize results\n",
    "        iteration_result={}\n",
    "\n",
    "        # Saving traces\n",
    "        iteration_result['reward_rec'] = np.array(reward_rec)\n",
    "        iteration_result['ep_done_rec'] = np.array(ep_done_rec)\n",
    "        iteration_result['action_log'] = np.array(env.action_log)\n",
    "        iteration_result['sense_dc_log'] = np.array(env.sense_dc_log)\n",
    "        iteration_result['env_log'] = np.array(env.env_log)\n",
    "        iteration_result['eno_log'] = np.array(env.eno_log)\n",
    "        iteration_result['sense_reward_log'] = np.array(env.sense_reward_log)\n",
    "        iteration_result['enp_reward_log'] = np.array(env.enp_reward_log)\n",
    "\n",
    "        # Summarizing results\n",
    "        env_log = iteration_result['env_log']\n",
    "\n",
    "        # Get henergy metrics\n",
    "        henergy_rec = env_log[:,1]\n",
    "        avg_henergy = henergy_rec.mean()\n",
    "        iteration_result['avg_henergy'] = avg_henergy\n",
    "\n",
    "        # Get req metrics\n",
    "        req_rec = env_log[:,5]\n",
    "        avg_req = req_rec.mean()            \n",
    "        iteration_result['avg_req'] = avg_req\n",
    "\n",
    "        # Get reward metrics\n",
    "        # In this case, the reward metrics directly reflect the conformity\n",
    "        reward_rec = iteration_result['reward_rec']\n",
    "        # negative rewards = -1000 correspond to downtimes\n",
    "        # To find average reward, remove negative values\n",
    "        index = np.argwhere(reward_rec<0)\n",
    "        rwd_rec = np.delete(reward_rec, index)\n",
    "        avg_rwd = rwd_rec.mean()\n",
    "        iteration_result['avg_rwd'] = avg_rwd\n",
    "\n",
    "        # Get downtime metrics\n",
    "        ep_done_rec =  iteration_result['ep_done_rec']\n",
    "        downtimes = np.count_nonzero(ep_done_rec[:-1] < ep_done_rec[1:]) - 1 # last env done is excluded\n",
    "        iteration_result['downtimes'] = downtimes\n",
    "\n",
    "        # Get ENP metrics\n",
    "        eno_log = iteration_result['eno_log']\n",
    "        enp_log = []\n",
    "        enp_log.append(eno_log[0])\n",
    "        for t in range(1,len(eno_log)):\n",
    "            enp = enp_log[-1] + eno_log[t]\n",
    "            enp = np.clip(enp,0,1)\n",
    "            enp_log.append(enp)\n",
    "        iteration_result['enp_log'] = np.array(enp_log)\n",
    "\n",
    "#         print(\"Year:\\t\", year)            \n",
    "        exp_test_log[env_location][year] = iteration_result\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "################################################################################\n",
    "# Save Test Results\n",
    "np.save(test_log_file, exp_test_log)\n",
    "\n",
    "# In[20]:\n",
    "################################################################################\n",
    "# summarize metrics and display\n",
    "print(\"\\n\\n***TEST RESULTS****\")\n",
    "print(\"Tag:\", tag)\n",
    "print(\"Seed:\", seed)\n",
    "\n",
    "\n",
    "print(\"LOCATION\".ljust(12), \"YEAR\".ljust(6), \"HMEAN\".ljust(8), \"REQ_MEAN\".ljust(8), \"AVG_DC\".ljust(8), \n",
    "      \"SNS_RWD\".ljust(8), \"ENP_RWD\".ljust(8), \"AVG_RWD\".ljust(8), \"DOWNTIMES\".ljust(9))\n",
    "\n",
    "exp_result = exp_test_log\n",
    "location_list = list(exp_result.keys())\n",
    "for location in location_list:\n",
    "    yr_list = list(exp_result[location].keys())\n",
    "    for year in yr_list:\n",
    "        run_log = exp_result[location][year]\n",
    "        # Print summarized metrics\n",
    "        print(location.ljust(12), year, end=' ')\n",
    "        sense_avg_rwd = run_log['sense_reward_log'].mean()\n",
    "        enp_avg_rwd = run_log['enp_reward_log'].mean()\n",
    "    \n",
    "        average_rwd = run_log['avg_rwd']\n",
    "        total_downtimes = run_log['downtimes']\n",
    "        hmean = run_log['avg_henergy']\n",
    "        reqmean = run_log['avg_req']\n",
    "        sense_dc_mean = run_log['sense_dc_log'].mean()\n",
    "\n",
    "        print(f'{hmean:7.3f}',end='  ')\n",
    "        print(f'{reqmean:7.3f}',end='  ')\n",
    "        print(f'{sense_dc_mean:7.3f}',end='  ')\n",
    "        print(f'{sense_avg_rwd:7.3f}',end='  ')\n",
    "        print(f'{enp_avg_rwd:7.3f}',end='  ')\n",
    "        print(f'{average_rwd:7.3f}',end='  ')\n",
    "        print(f'{total_downtimes:5d}',end='  ')\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
