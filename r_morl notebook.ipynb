{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# python ./morl_runtime.py --env=csense --gamma=0.997 --noise=0.7 --pref=0.5 --seed=123\n",
    "\n",
    "# In[1]:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import random\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40) # remove gym warning about float32 bound box precision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import common.env_lib\n",
    "from common.env_utils import sorl_plot\n",
    "from common.rl_lib import ReplayBuffer, OUNoise, ENoise, ValueNetwork, PolicyNetwork, ddpg_update, calc_values_of_states\n",
    "\n",
    "\n",
    "# # In[3]:\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--env\", default=\"csense\", type=str, help=\"Environment. Specified in ./common/env_lib.py\")\n",
    "# parser.add_argument(\"--gamma\", default=0.996, type=float, help=\"Discount Factor\")\n",
    "# parser.add_argument(\"--noise\", default=0.8, type=float, help=\"Action Noise\")\n",
    "# parser.add_argument(\"--seed\", default=238, type=int, help=\"Set seed [default: 238]\")\n",
    "\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "# arguments\n",
    "seed      = 752389#args.seed\n",
    "env_name  = \"renpv2\"#args.env\n",
    "GAMMA     = 0.997#args.gamma\n",
    "max_noise = 0.3#args.noise\n",
    "pref     = 0.5#args.pref\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# set seed\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "################################################################################\n",
    "# If using GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "################################################################################\n",
    "# Setup experiment parameters\n",
    "timeslots_per_day = 24\n",
    "REQ_TYPE = \"random\"\n",
    "\n",
    "cur_folder = os.getcwd()\n",
    "model_folder = os.path.join(cur_folder,\"models\")\n",
    "\n",
    "# Type of RL training used for the model that we are going to load\n",
    "training_experiment = \"base_g\" + str(GAMMA) + \"-n\" + str(max_noise)\n",
    "################################################################################\n",
    "\n",
    "# Tags for ENP model\n",
    "enp_env_tag = env_name + '_t' + str(timeslots_per_day) + '_' + REQ_TYPE\n",
    "# name of folder to load model from\n",
    "enp_model_folder = enp_env_tag  + \"-\" + training_experiment \n",
    "\n",
    "# model filename\n",
    "enp_model_tag   = enp_model_folder +'-' + str(seed)\n",
    "enp_pmodel_file = os.path.join(model_folder, enp_model_folder, (enp_model_tag + \"-policy.pt\"))\n",
    "enp_qmodel_file = os.path.join(model_folder, enp_model_folder, (enp_model_tag + \"-value.pt\"))\n",
    "################################################################################\n",
    "\n",
    "# Tags for sense model\n",
    "# csense for cenp\n",
    "# rsense for renp\n",
    "sense_env_tag = env_name[0]+ \"sense\" + '_t' + str(timeslots_per_day) + '_' + REQ_TYPE\n",
    "# name of folder to load model from\n",
    "sense_model_folder = sense_env_tag  + \"-\" + training_experiment \n",
    "\n",
    "# model filename\n",
    "sense_model_tag   = sense_model_folder + '-' + str(seed)\n",
    "sense_pmodel_file = os.path.join(model_folder, sense_model_folder, (sense_model_tag + \"-policy.pt\"))\n",
    "sense_qmodel_file = os.path.join(model_folder, sense_model_folder, (sense_model_tag + \"-value.pt\"))\n",
    "\n",
    "################################################################################\n",
    "# Setting up runtime environment base\n",
    "experiment = \"morl_runtime_g\" + str(GAMMA) + \"-n\" + str(max_noise) + \"-p\" + str(pref)\n",
    "#env: cenp -> cenpsense\n",
    "env = eval(\"common.env_lib.\"+env_name+\"sense\"+\"()\")\n",
    "env.set_pref(pref) # preference over actions\n",
    "################################################################################\n",
    "\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "hidden_dim = 256\n",
    "\n",
    "# Make model\n",
    "sense_pnet = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "enp_pnet = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "sense_qnet = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "enp_qnet = ValueNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "sense_pnet.load_state_dict(torch.load(sense_pmodel_file))\n",
    "sense_pnet.eval()\n",
    "\n",
    "sense_qnet.load_state_dict(torch.load(sense_qmodel_file))\n",
    "sense_qnet.eval()\n",
    "\n",
    "enp_pnet.load_state_dict(torch.load(enp_pmodel_file))\n",
    "enp_pnet.eval()\n",
    "\n",
    "enp_qnet.load_state_dict(torch.load(enp_qmodel_file))\n",
    "enp_qnet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment TAG:  renpv2_t24_random-morl_runtime_g0.997-n0.3-p0.5-752389\n"
     ]
    }
   ],
   "source": [
    "# Tags for morl_runtime experiment\n",
    "env_tag = env_name + '_t' + str(timeslots_per_day) + '_' + REQ_TYPE\n",
    "model_tag = experiment +'-'+str(seed)\n",
    "\n",
    "# experiment tag\n",
    "# name of folder to save models and results\n",
    "exp_tag = env_tag  + \"-\" + experiment \n",
    "\n",
    "# experiment+seed tag\n",
    "# tensorboard tag / model filename\n",
    "tag     = env_tag  + \"-\" + experiment +'-'+str(seed) \n",
    "print(\"Experiment TAG: \",tag)\n",
    "\n",
    "# Folder/file to save test results\n",
    "test_results_folder = os.path.join(cur_folder,\"results\", exp_tag, \"test\")\n",
    "if not os.path.exists(test_results_folder): \n",
    "        os.makedirs(test_results_folder) \n",
    "test_log_file = os.path.join(test_results_folder, tag + '-test.npy')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "***TEST RESULTS****\n",
      "Tag: renpv2_t24_random-morl_runtime_g0.997-n0.3-p0.5-752389\n",
      "Seed: 752389\n",
      "LOCATION     YEAR   HMEAN    REQ_MEAN AVG_DC   SNS_RWD  ENP_RWD  AVG_RWD  DOWNTIMES\n",
      "tokyo        1995   0.127    0.136    0.010    0.032    0.000    0.016      0  \n",
      "tokyo        1996   0.131    0.137    0.011    0.036    0.000    0.018      0  \n",
      "tokyo        1997   0.131    0.137    0.009    0.029    0.000    0.015      0  \n",
      "tokyo        1998   0.113    0.132    0.008    0.028    0.000    0.014      0  \n",
      "tokyo        1999   0.130    0.136    0.009    0.031    0.000    0.016      0  \n",
      "tokyo        2000   0.132    0.137    0.010    0.032    0.000    0.016      0  \n",
      "tokyo        2001   0.132    0.137    0.009    0.030    0.000    0.015      0  \n",
      "tokyo        2002   0.133    0.137    0.010    0.034    0.000    0.017      0  \n",
      "tokyo        2003   0.123    0.135    0.009    0.030    0.000    0.015      0  \n",
      "tokyo        2004   0.141    0.139    0.009    0.030    0.000    0.015      0  \n",
      "tokyo        2005   0.133    0.137    0.009    0.031    0.000    0.015      0  \n",
      "tokyo        2006   0.119    0.133    0.008    0.030    0.000    0.015      0  \n",
      "tokyo        2007   0.136    0.138    0.010    0.031    0.000    0.016      0  \n",
      "tokyo        2008   0.131    0.137    0.008    0.027    0.000    0.013      0  \n",
      "tokyo        2009   0.130    0.136    0.009    0.030    0.000    0.015      0  \n",
      "tokyo        2010   0.138    0.139    0.008    0.028    0.000    0.014      0  \n",
      "tokyo        2011   0.142    0.140    0.010    0.032    0.000    0.016      0  \n",
      "tokyo        2012   0.144    0.140    0.008    0.029    0.000    0.014      0  \n",
      "tokyo        2013   0.148    0.141    0.010    0.032    0.000    0.016      0  \n",
      "tokyo        2014   0.143    0.140    0.010    0.031    0.000    0.016      0  \n",
      "tokyo        2015   0.136    0.138    0.010    0.033    0.000    0.017      0  \n",
      "tokyo        2016   0.132    0.137    0.010    0.034    0.000    0.017      0  \n",
      "tokyo        2017   0.141    0.140    0.009    0.030    0.000    0.015      0  \n",
      "tokyo        2018   0.145    0.141    0.010    0.031    0.000    0.016      0  \n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "env_location_list = ['tokyo']#['tokyo','wakkanai','minamidaito']\n",
    "START_YEAR = 1995\n",
    "NO_OF_YEARS = 24\n",
    "# timeslots_per_day = 24\n",
    "# REQ_TYPE = \"random\"\n",
    "prediction_horizon = 10*timeslots_per_day\n",
    "henergy_mean= 0.13904705134356052 # 10yr hmean for tokyo\n",
    "\n",
    "exp_test_log = {} # dictionary to store test results\n",
    "\n",
    "for env_location in env_location_list:\n",
    "#     print(env_location)\n",
    "    exp_test_log[env_location] = {}\n",
    "    for year in range(START_YEAR, START_YEAR+NO_OF_YEARS):\n",
    "        exp_test_log[env_location][year]={}\n",
    "        \n",
    "        env.set_env(env_location,year, timeslots_per_day, \n",
    "                    REQ_TYPE, offset=timeslots_per_day/2,\n",
    "                    p_horizon=prediction_horizon,\n",
    "                    hmean=henergy_mean)    \n",
    "        state = env.reset()\n",
    "        reward_rec = []\n",
    "        \n",
    "        intrp_dc_rec = []\n",
    "        intrp_sense_value_rec = []\n",
    "        intrp_enp_value_rec = []\n",
    "        intrp_final_value_rec = []\n",
    "        \n",
    "        ep_done_rec = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            if env.RECOVERY_MODE:\n",
    "                no_action = 0            \n",
    "                next_state, reward, done, _ = env.step(no_action)       \n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    req_obs = state[-1] # request from server\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "                    # sense_model# sense_model: get action and q-value\n",
    "                    # convert action to DC\n",
    "                    sense_action_tensor = sense_pnet(state_tensor)\n",
    "                    sense_value = sense_qnet(state_tensor, sense_action_tensor).cpu().item()\n",
    "                    raw_sense_action = sense_action_tensor.cpu().item()\n",
    "                    sense_dc = (raw_sense_action*0.4 + env.MIN_DC)\n",
    "                    \n",
    "                    # enp_model: get action and q-value\n",
    "                    # convert action to DC\n",
    "                    enp_action_tensor = enp_pnet(state_tensor)\n",
    "                    enp_value = enp_qnet(state_tensor, enp_action_tensor).cpu().item()\n",
    "                    raw_enp_action = enp_action_tensor.cpu().item()\n",
    "                    enp_dc = (raw_enp_action*0.4 + env.MIN_DC)\n",
    "\n",
    "                # Find two intermediate DC\n",
    "                intrp_dc = np.linspace(start=min(sense_dc,enp_dc), \n",
    "                                       stop=max(sense_dc,enp_dc), \n",
    "                                       num=4)\n",
    "                intrp_dc_rec.append(intrp_dc)\n",
    "                                \n",
    "                # Convert intrp_dc to action\n",
    "                intrp_action_tensor = torch.FloatTensor(intrp_dc/req_obs).unsqueeze(1).to(device)\n",
    "                batch_state_tensor = torch.FloatTensor([state]*len(intrp_dc)).to(device)\n",
    "                \n",
    "                # Get the q-values from sense and enp models for the intrp_dc\n",
    "                intrp_sense_value = sense_qnet(batch_state_tensor, intrp_action_tensor).cpu().detach().numpy().squeeze()\n",
    "                intrp_enp_value = enp_qnet(batch_state_tensor, intrp_action_tensor).cpu().detach().numpy().squeeze()\n",
    "                intrp_sense_value_rec.append(intrp_sense_value)\n",
    "                intrp_enp_value_rec.append(intrp_enp_value)\n",
    "                \n",
    "                # Calculate the node utility for all intrp_actions\n",
    "                final_value = pref*intrp_sense_value + (1-pref)*intrp_enp_value\n",
    "                intrp_final_value_rec.append(final_value)\n",
    "                \n",
    "                # final_action is the action with the maximum final_value\n",
    "                final_dc = intrp_dc[final_value.argmax()]\n",
    "                final_action = (final_dc - env.MIN_DC)/0.4\n",
    "                \n",
    "                # Execute action\n",
    "                next_state, reward, done, _ = env.step(final_action)\n",
    "\n",
    "            reward_rec.append(reward)\n",
    "            ep_done = done or env.RECOVERY_MODE\n",
    "            ep_done_rec.append(ep_done)\n",
    "            state = next_state\n",
    "\n",
    "        # Log the traces and summarize results\n",
    "        iteration_result={}\n",
    "\n",
    "        # Saving traces\n",
    "        iteration_result['reward_rec'] = np.array(reward_rec)\n",
    "        iteration_result['ep_done_rec'] = np.array(ep_done_rec)\n",
    "        iteration_result['action_log'] = np.array(env.action_log)\n",
    "        iteration_result['sense_dc_log'] = np.array(env.sense_dc_log)\n",
    "        iteration_result['env_log'] = np.array(env.env_log)\n",
    "        iteration_result['eno_log'] = np.array(env.eno_log)\n",
    "        iteration_result['sense_reward_log'] = np.array(env.sense_reward_log)\n",
    "        iteration_result['enp_reward_log'] = np.array(env.enp_reward_log)\n",
    "        \n",
    "        iteration_result['intrp_dc_rec'] = intrp_dc_rec\n",
    "        iteration_result['intrp_sense_value_rec'] = intrp_sense_value_rec\n",
    "        iteration_result['intrp_enp_value_rec'] = intrp_enp_value_rec\n",
    "        iteration_result['intrp_final_value_rec'] = intrp_final_value_rec\n",
    "\n",
    "        # Summarizing results\n",
    "        env_log = iteration_result['env_log']\n",
    "\n",
    "        # Get henergy metrics\n",
    "        henergy_rec = env_log[:,1]\n",
    "        avg_henergy = henergy_rec.mean()\n",
    "        iteration_result['avg_henergy'] = avg_henergy\n",
    "\n",
    "        # Get req metrics\n",
    "        req_rec = env_log[:,5]\n",
    "        avg_req = req_rec.mean()            \n",
    "        iteration_result['avg_req'] = avg_req\n",
    "\n",
    "        # Get reward metrics\n",
    "        # In this case, the reward metrics directly reflect the conformity\n",
    "        reward_rec = iteration_result['reward_rec']\n",
    "        # negative rewards = -1000 correspond to downtimes\n",
    "        # To find average reward, remove negative values\n",
    "        index = np.argwhere(reward_rec<0)\n",
    "        rwd_rec = np.delete(reward_rec, index)\n",
    "        avg_rwd = rwd_rec.mean()\n",
    "        iteration_result['avg_rwd'] = avg_rwd\n",
    "\n",
    "        # Get downtime metrics\n",
    "        ep_done_rec =  iteration_result['ep_done_rec']\n",
    "        downtimes = np.count_nonzero(ep_done_rec[:-1] < ep_done_rec[1:]) - 1 # last env done is excluded\n",
    "        iteration_result['downtimes'] = downtimes\n",
    "\n",
    "        # Get ENP metrics\n",
    "        eno_log = iteration_result['eno_log']\n",
    "        enp_log = []\n",
    "        enp_log.append(eno_log[0])\n",
    "        for t in range(1,len(eno_log)):\n",
    "            enp = enp_log[-1] + eno_log[t]\n",
    "            enp = np.clip(enp,0,1)\n",
    "            enp_log.append(enp)\n",
    "        iteration_result['enp_log'] = np.array(enp_log)\n",
    "\n",
    "#         print(\"Year:\\t\", year)            \n",
    "        exp_test_log[env_location][year] = iteration_result\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "################################################################################\n",
    "# Save Test Results\n",
    "np.save(test_log_file, exp_test_log)\n",
    "\n",
    "# In[20]:\n",
    "################################################################################\n",
    "# summarize metrics and display\n",
    "print(\"\\n\\n***TEST RESULTS****\")\n",
    "print(\"Tag:\", tag)\n",
    "print(\"Seed:\", seed)\n",
    "\n",
    "\n",
    "print(\"LOCATION\".ljust(12), \"YEAR\".ljust(6), \"HMEAN\".ljust(8), \"REQ_MEAN\".ljust(8), \"AVG_DC\".ljust(8), \n",
    "      \"SNS_RWD\".ljust(8), \"ENP_RWD\".ljust(8), \"AVG_RWD\".ljust(8), \"DOWNTIMES\".ljust(9))\n",
    "\n",
    "exp_result = exp_test_log\n",
    "location_list = list(exp_result.keys())\n",
    "for location in location_list:\n",
    "    yr_list = list(exp_result[location].keys())\n",
    "    for year in yr_list:\n",
    "        run_log = exp_result[location][year]\n",
    "        # Print summarized metrics\n",
    "        print(location.ljust(12), year, end=' ')\n",
    "        sense_avg_rwd = run_log['sense_reward_log'].mean()\n",
    "        enp_avg_rwd = run_log['enp_reward_log'].mean()\n",
    "    \n",
    "        average_rwd = run_log['avg_rwd']\n",
    "        total_downtimes = run_log['downtimes']\n",
    "        hmean = run_log['avg_henergy']\n",
    "        reqmean = run_log['avg_req']\n",
    "        sense_dc_mean = run_log['sense_dc_log'].mean()\n",
    "\n",
    "        print(f'{hmean:7.3f}',end='  ')\n",
    "        print(f'{reqmean:7.3f}',end='  ')\n",
    "        print(f'{sense_dc_mean:7.3f}',end='  ')\n",
    "        print(f'{sense_avg_rwd:7.3f}',end='  ')\n",
    "        print(f'{enp_avg_rwd:7.3f}',end='  ')\n",
    "        print(f'{average_rwd:7.3f}',end='  ')\n",
    "        print(f'{total_downtimes:5d}',end='  ')\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
